FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=5 10 3 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (5, 5, 5.00000000000000000000e-01) (5, 5, 5.00000000000000000000e-01) (5, 5, 5.00000000000000000000e-01) (5, 5, 5.00000000000000000000e-01) (5, 5, 5.00000000000000000000e-01) (5, 5, 5.00000000000000000000e-01) (5, 5, 5.00000000000000000000e-01) (5, 5, 5.00000000000000000000e-01) (5, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (10, 5, 5.00000000000000000000e-01) (10, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -6.49144411087036132812e+00) (1, 3.46015281677246093750e+01) (2, 1.07136035156250000000e+03) (3, -1.31625930786132812500e+02) (4, 2.33412265777587890625e+00) (0, 2.35496196746826171875e+01) (1, -9.75357284545898437500e+01) (2, 1.50000000000000000000e+03) (3, 2.37774978637695312500e+02) (4, 3.75248026847839355469e+00) (0, -1.98493614196777343750e+01) (1, -2.56862060546875000000e+02) (2, 2.33049392700195312500e+02) (3, -7.83681259155273437500e+01) (4, -3.32194924354553222656e+00) (0, -1.87254953384399414062e+00) (1, -2.18787017822265625000e+02) (2, 1.50000000000000000000e+03) (3, -2.56843986511230468750e+01) (4, -2.21239715814590454102e-01) (0, -1.88471496105194091797e+00) (1, -3.03671855926513671875e+01) (2, 1.14024848937988281250e+02) (3, -6.36834869384765625000e+01) (4, 1.74555674195289611816e-01) (0, -1.96985185146331787109e+00) (1, -1.02579555511474609375e+01) (2, 2.47412277221679687500e+02) (3, 7.96744079589843750000e+01) (4, 7.40685611963272094727e-02) (0, -1.12188444137573242188e+01) (1, 3.63406829833984375000e+01) (2, 1.44477172851562500000e+03) (3, 1.19215808105468750000e+03) (4, -7.82002329826354980469e-01) (0, 2.81098634004592895508e-01) (1, 1.45906171798706054688e+01) (2, 1.71238372802734375000e+02) (3, 1.01609733581542968750e+02) (4, -1.98281869292259216309e-01) (0, 4.88697357177734375000e+01) (1, -2.48861312866210937500e+01) (2, 1.50000000000000000000e+03) (3, -8.54967651367187500000e+01) (4, 2.46062016487121582031e+00) (5, -9.26972770690917968750e+00) (6, -9.71799373626708984375e+00) (7, -1.70095577239990234375e+01) (8, 9.98303985595703125000e+00) (9, 9.77485466003417968750e+00) (10, -1.71696853637695312500e+01) (11, 9.55690479278564453125e+00) (12, -1.41326951980590820312e+01) (13, 4.26251935958862304688e+00) (14, -1.45218181610107421875e+00) (5, 9.78158473968505859375e+00) (6, 9.70283222198486328125e+00) (7, 1.70052490234375000000e+01) (8, -9.95971298217773437500e+00) (9, -1.26414947509765625000e+01) (10, 2.03781852722167968750e+01) (11, -1.03945474624633789062e+01) (12, 1.12709989547729492188e+01) (13, -3.90895032882690429688e+00) (14, 1.40448963642120361328e+00) 
