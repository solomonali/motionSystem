FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=3 10 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (3, 5, 5.00000000000000000000e-01) (3, 5, 5.00000000000000000000e-01) (3, 5, 5.00000000000000000000e-01) (3, 5, 5.00000000000000000000e-01) (3, 5, 5.00000000000000000000e-01) (3, 5, 5.00000000000000000000e-01) (3, 5, 5.00000000000000000000e-01) (3, 5, 5.00000000000000000000e-01) (3, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (10, 5, 5.00000000000000000000e-01) (10, 5, 5.00000000000000000000e-01) (10, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.48462712764739990234e-01) (1, 4.06646817922592163086e-01) (2, 3.44400584697723388672e-01) (0, -1.97637116909027099609e+00) (1, 1.98882079124450683594e+00) (2, 2.00118756294250488281e+00) (0, -1.96252870559692382812e+00) (1, 2.05203366279602050781e+00) (2, 2.07128047943115234375e+00) (0, 2.01336979866027832031e+00) (1, -2.07501816749572753906e+00) (2, -2.05650281906127929688e+00) (0, 2.90587395429611206055e-01) (1, -1.06037452816963195801e-01) (2, -1.97789907455444335938e-01) (0, -6.01271867752075195312e-01) (1, 6.03115081787109375000e-01) (2, 7.49687850475311279297e-01) (0, 2.07136720418930053711e-02) (1, 1.79012924432754516602e-01) (2, 2.20252498984336853027e-01) (0, 2.24805742502212524414e-01) (1, 4.73327003419399261475e-02) (2, 1.86005048453807830811e-02) (0, 2.36560538411140441895e-01) (1, 2.40699835121631622314e-02) (2, 2.02564857900142669678e-02) (3, -2.18890890479087829590e-01) (4, -2.04838156700134277344e+00) (5, -2.06173014640808105469e+00) (6, 2.03438234329223632812e+00) (7, 2.60441631078720092773e-01) (8, -6.80444657802581787109e-01) (9, -1.55303075909614562988e-01) (10, -1.95525214076042175293e-02) (11, 1.14895842969417572021e-01) (12, -1.88999915122985839844e+00) (3, -1.56072214245796203613e-01) (4, -2.00430774688720703125e+00) (5, -1.89860939979553222656e+00) (6, 1.94119513034820556641e+00) (7, 2.33952924609184265137e-01) (8, -8.70285272598266601562e-01) (9, -1.02303355932235717773e-01) (10, 1.68299466371536254883e-01) (11, 4.22328040003776550293e-02) (12, -1.92461383342742919922e+00) (3, 3.22373807430267333984e-01) (4, 1.90540087223052978516e+00) (5, 2.00297880172729492188e+00) (6, -2.00789928436279296875e+00) (7, -5.48324584960937500000e-02) (8, 8.54592144489288330078e-01) (9, 3.79637777805328369141e-01) (10, 2.29137688875198364258e-01) (11, 9.61983427405357360840e-02) (12, 2.04184484481811523438e+00) 
